# Project Assessment

1. Do you understand the steps involved in creating and deploying an LTR model?Â  Name them and describe what each step does in your own words.

Yes. The steps are the following:
- Create a featureset: involves defining all the features that can be used for training, and that may be used for the final model (as the model can discard useless features);
- Collect statistics for documents/queries: is the process of gathering the features of all relevant documents for all relevant queries, for training;
- Train the model: using a golden set of documents and the features obtained in the previous step, creates a model using whichever machine learning library you're using;
- Deploy the model in OpenSearch: involves PUTting the trained model in OpenSearch for it to be able to be used;
- Test the model: using another golden set for training, test whether the model yields satisfactory results or not.

2. What is a feature and featureset?

A _feature_ is some data or metadata related to a specific document and may also be related to a specific query, if it's a query-dependent signal. A _featureset_ is the set of such features, that can all be acessed in a sltr query.

3. What is the difference between precision and recall?

- Precision is a metric that determines the percentage of results in a top-k result set that are considered relevant; 
- Recall is the metric that measures the amount of relevant documents that were returned in the top-k result over the total amount of relevant documents that exist.

Their difference, then, is what they tell us about a result set. One tells the percentage of correctness of the k results (precision), while the other is used to measure how much the result set can be improved.

4. What are some of the traps associated with using click data in your model?

Click data, by definition, relies on past results. That means that, if there is a (theoretical) _super_ relevant document that was never returned in any of the baseline searches, there will be also no click data for that document, which means that the model won't be able to boost it to any degree of relevance using click data.

5. What are some of the ways we are faking our data and how would you prevent that in your application?

We are generating fake clicks and impressions for our documents and query sets. This means that a model generated by this application will more likely than not be a bad model. In a real application, there could be logging of both impressions and clicks, so that it doesn't need to be generated randomly.

Also for testing the model, A/B testing can be used instead of a similar dataset to the one used for training.

6. What is target leakage and why is it a bad thing?

Target leakage is when, during the training process, the model library gets a "spoiler" of the expected results of the training sessions; this means that, using that information, it can be overfitted to the test data, and thus, behave badly in real use cases.

7. When can using prior history cause problems in search and LTR?

It can cause problems when:

- The prior data is bad: if the ranking that was in place when the dataset was gathered is really bad, the model will only learn how to figure out which results are the less bad ones, instead of learning the good results;
- The data is in constant change: if the field where your searches are done is one of constant evolution, where new documents tend to be viewed as more relevant than dated ones, prior click data will only add some skew to the model, since new documents won't have this data.